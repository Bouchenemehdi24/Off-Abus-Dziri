{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "cLQJWt72Tkcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "import sys, os, io\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import optuna\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mlxtend.plotting import plot_learning_curves\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-29T19:48:34.063507Z",
          "iopub.execute_input": "2022-09-29T19:48:34.064202Z",
          "iopub.status.idle": "2022-09-29T19:48:34.848580Z",
          "shell.execute_reply.started": "2022-09-29T19:48:34.064162Z",
          "shell.execute_reply": "2022-09-29T19:48:34.847651Z"
        },
        "trusted": true,
        "id": "IkBOjsWlTkc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "OsD9epG2Tkc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "_CORPUS_PATH = ''\n",
        "#_CORPUS_PATH = './corpus/'\n",
        "_TOP_K_FEATURES = 1000\n",
        "indexes = []\n",
        "vocab_size = 0\n",
        "\n",
        "\n",
        "class ARLSTem:\n",
        "    \"\"\"\n",
        "    ARLSTem stemmer : a light Arabic Stemming algorithm without any dictionary.\n",
        "    Department of Telecommunication & Information Processing. USTHB University,\n",
        "    Algiers, Algeria.\n",
        "    ARLSTem.stem(token) returns the Arabic stem for the input token.\n",
        "    The ARLSTem Stemmer requires that all tokens are encoded using Unicode\n",
        "    encoding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # different Alif with hamza\n",
        "        self.re_hamzated_alif = re.compile(r\"[\\u0622\\u0623\\u0625]\")\n",
        "        self.re_alifMaqsura = re.compile(r\"[\\u0649]\")\n",
        "        self.re_diacritics = re.compile(r\"[\\u064B-\\u065F]\")\n",
        "\n",
        "        # Alif Laam, Laam Laam, Fa Laam, Fa Ba\n",
        "        self.pr2 = [\"\\u0627\\u0644\", \"\\u0644\\u0644\", \"\\u0641\\u0644\", \"\\u0641\\u0628\"]\n",
        "        # Ba Alif Laam, Kaaf Alif Laam, Waaw Alif Laam\n",
        "        self.pr3 = [\"\\u0628\\u0627\\u0644\", \"\\u0643\\u0627\\u0644\", \"\\u0648\\u0627\\u0644\"]\n",
        "        # Fa Laam Laam, Waaw Laam Laam\n",
        "        self.pr32 = [\"\\u0641\\u0644\\u0644\", \"\\u0648\\u0644\\u0644\"]\n",
        "        # Fa Ba Alif Laam, Waaw Ba Alif Laam, Fa Kaaf Alif Laam\n",
        "        self.pr4 = [\n",
        "            \"\\u0641\\u0628\\u0627\\u0644\",\n",
        "            \"\\u0648\\u0628\\u0627\\u0644\",\n",
        "            \"\\u0641\\u0643\\u0627\\u0644\",\n",
        "        ]\n",
        "\n",
        "        # Kaf Yaa, Kaf Miim\n",
        "        self.su2 = [\"\\u0643\\u064A\", \"\\u0643\\u0645\"]\n",
        "        # Ha Alif, Ha Miim\n",
        "        self.su22 = [\"\\u0647\\u0627\", \"\\u0647\\u0645\"]\n",
        "        # Kaf Miim Alif, Kaf Noon Shadda\n",
        "        self.su3 = [\"\\u0643\\u0645\\u0627\", \"\\u0643\\u0646\\u0651\"]\n",
        "        # Ha Miim Alif, Ha Noon Shadda\n",
        "        self.su32 = [\"\\u0647\\u0645\\u0627\", \"\\u0647\\u0646\\u0651\"]\n",
        "\n",
        "        # Alif Noon, Ya Noon, Waaw Noon\n",
        "        self.pl_si2 = [\"\\u0627\\u0646\", \"\\u064A\\u0646\", \"\\u0648\\u0646\"]\n",
        "        # Taa Alif Noon, Taa Ya Noon\n",
        "        self.pl_si3 = [\"\\u062A\\u0627\\u0646\", \"\\u062A\\u064A\\u0646\"]\n",
        "\n",
        "        # Alif Noon, Waaw Noon\n",
        "        self.verb_su2 = [\"\\u0627\\u0646\", \"\\u0648\\u0646\"]\n",
        "        # Siin Taa, Siin Yaa\n",
        "        self.verb_pr2 = [\"\\u0633\\u062A\", \"\\u0633\\u064A\"]\n",
        "        # Siin Alif, Siin Noon\n",
        "        self.verb_pr22 = [\"\\u0633\\u0627\", \"\\u0633\\u0646\"]\n",
        "        # Lam Noon, Lam Taa, Lam Yaa, Lam Hamza\n",
        "        self.verb_pr33 = [\n",
        "            \"\\u0644\\u0646\",\n",
        "            \"\\u0644\\u062A\",\n",
        "            \"\\u0644\\u064A\",\n",
        "            \"\\u0644\\u0623\",\n",
        "        ]\n",
        "        # Taa Miim Alif, Taa Noon Shadda\n",
        "        self.verb_suf3 = [\"\\u062A\\u0645\\u0627\", \"\\u062A\\u0646\\u0651\"]\n",
        "        # Noon Alif, Taa Miim, Taa Alif, Waaw Alif\n",
        "        self.verb_suf2 = [\n",
        "            \"\\u0646\\u0627\",\n",
        "            \"\\u062A\\u0645\",\n",
        "            \"\\u062A\\u0627\",\n",
        "            \"\\u0648\\u0627\",\n",
        "        ]\n",
        "        # Taa, Alif, Noon\n",
        "        self.verb_suf1 = [\"\\u062A\", \"\\u0627\", \"\\u0646\"]\n",
        "\n",
        "\n",
        "    def stem(self, token):\n",
        "        \"\"\"\n",
        "        call this function to get the word's stem based on ARLSTem .\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if token is None:\n",
        "                raise ValueError(\n",
        "                    \"The word could not be stemmed, because \\\n",
        "                                 it is empty !\"\n",
        "                )\n",
        "            # remove Arabic diacritics and replace some letters with others\n",
        "            token = self.norm(token)\n",
        "            # strip common prefixes of the nouns\n",
        "            pre = self.pref(token)\n",
        "            if pre is not None:\n",
        "                token = pre\n",
        "            # strip the suffixes which are common to nouns and verbs\n",
        "            token = self.suff(token)\n",
        "            # transform a plural noun to a singular noun\n",
        "            ps = self.plur2sing(token)\n",
        "            if ps is None:\n",
        "                # transform from the feminine form to the masculine form\n",
        "                fm = self.fem2masc(token)\n",
        "                if fm is not None:\n",
        "                    return fm\n",
        "                else:\n",
        "                    if pre is None:  # if the prefixes are not stripped\n",
        "                        # strip the verb prefixes and suffixes\n",
        "                        return self.verb(token)\n",
        "            else:\n",
        "                return ps\n",
        "            return token\n",
        "        except ValueError as e:\n",
        "            print(e)\n",
        "\n",
        "\n",
        "    def norm(self, token):\n",
        "        \"\"\"\n",
        "        normalize the word by removing diacritics, replacing hamzated Alif\n",
        "        with Alif replacing AlifMaqsura with Yaa and removing Waaw at the\n",
        "        beginning.\n",
        "        \"\"\"\n",
        "        # strip Arabic diacritics\n",
        "        token = self.re_diacritics.sub(\"\", token)\n",
        "        # replace Hamzated Alif with Alif bare\n",
        "        token = self.re_hamzated_alif.sub(\"\\u0627\", token)\n",
        "        # replace alifMaqsura with Yaa\n",
        "        token = self.re_alifMaqsura.sub(\"\\u064A\", token)\n",
        "        # strip the Waaw from the word beginning if the remaining is 3 letters\n",
        "        # at least\n",
        "        if token.startswith(\"\\u0648\") and len(token) > 3:\n",
        "            token = token[1:]\n",
        "        return token\n",
        "\n",
        "    def pref(self, token):\n",
        "        \"\"\"\n",
        "        remove prefixes from the words' beginning.\n",
        "        \"\"\"\n",
        "        if len(token) > 5:\n",
        "            for p3 in self.pr3:\n",
        "                if token.startswith(p3):\n",
        "                    return token[3:]\n",
        "        if len(token) > 6:\n",
        "            for p4 in self.pr4:\n",
        "                if token.startswith(p4):\n",
        "                    return token[4:]\n",
        "        if len(token) > 5:\n",
        "            for p3 in self.pr32:\n",
        "                if token.startswith(p3):\n",
        "                    return token[3:]\n",
        "        if len(token) > 4:\n",
        "            for p2 in self.pr2:\n",
        "                if token.startswith(p2):\n",
        "                    return token[2:]\n",
        "\n",
        "\n",
        "    def suff(self, token):\n",
        "        \"\"\"\n",
        "        remove suffixes from the word's end.\n",
        "        \"\"\"\n",
        "        if token.endswith(\"\\u0643\") and len(token) > 3:\n",
        "            return token[:-1]\n",
        "        if len(token) > 4:\n",
        "            for s2 in self.su2:\n",
        "                if token.endswith(s2):\n",
        "                    return token[:-2]\n",
        "        if len(token) > 5:\n",
        "            for s3 in self.su3:\n",
        "                if token.endswith(s3):\n",
        "                    return token[:-3]\n",
        "        if token.endswith(\"\\u0647\") and len(token) > 3:\n",
        "            token = token[:-1]\n",
        "            return token\n",
        "        if len(token) > 4:\n",
        "            for s2 in self.su22:\n",
        "                if token.endswith(s2):\n",
        "                    return token[:-2]\n",
        "        if len(token) > 5:\n",
        "            for s3 in self.su32:\n",
        "                if token.endswith(s3):\n",
        "                    return token[:-3]\n",
        "        if token.endswith(\"\\u0646\\u0627\") and len(token) > 4:\n",
        "            return token[:-2]\n",
        "        return token\n",
        "\n",
        "\n",
        "    def fem2masc(self, token):\n",
        "        \"\"\"\n",
        "        transform the word from the feminine form to the masculine form.\n",
        "        \"\"\"\n",
        "        if token.endswith(\"\\u0629\") and len(token) > 3:\n",
        "            return token[:-1]\n",
        "\n",
        "\n",
        "    def plur2sing(self, token):\n",
        "        \"\"\"\n",
        "        transform the word from the plural form to the singular form.\n",
        "        \"\"\"\n",
        "        if len(token) > 4:\n",
        "            for ps2 in self.pl_si2:\n",
        "                if token.endswith(ps2):\n",
        "                    return token[:-2]\n",
        "        if len(token) > 5:\n",
        "            for ps3 in self.pl_si3:\n",
        "                if token.endswith(ps3):\n",
        "                    return token[:-3]\n",
        "        if len(token) > 3 and token.endswith(\"\\u0627\\u062A\"):\n",
        "            return token[:-2]\n",
        "        if len(token) > 3 and token.startswith(\"\\u0627\") and token[2] == \"\\u0627\":\n",
        "            return token[:2] + token[3:]\n",
        "        if len(token) > 4 and token.startswith(\"\\u0627\") and token[-2] == \"\\u0627\":\n",
        "            return token[1:-2] + token[-1]\n",
        "\n",
        "\n",
        "    def verb(self, token):\n",
        "        \"\"\"\n",
        "        stem the verb prefixes and suffixes or both\n",
        "        \"\"\"\n",
        "        vb = self.verb_t1(token)\n",
        "        if vb is not None:\n",
        "            return vb\n",
        "        vb = self.verb_t2(token)\n",
        "        if vb is not None:\n",
        "            return vb\n",
        "        vb = self.verb_t3(token)\n",
        "        if vb is not None:\n",
        "            return vb\n",
        "        vb = self.verb_t4(token)\n",
        "        if vb is not None:\n",
        "            return vb\n",
        "        vb = self.verb_t5(token)\n",
        "        if vb is not None:\n",
        "            return vb\n",
        "        return self.verb_t6(token)\n",
        "\n",
        "\n",
        "    def verb_t1(self, token):\n",
        "        \"\"\"\n",
        "        stem the present prefixes and suffixes\n",
        "        \"\"\"\n",
        "        if len(token) > 5 and token.startswith(\"\\u062A\"):  # Taa\n",
        "            for s2 in self.pl_si2:\n",
        "                if token.endswith(s2):\n",
        "                    return token[1:-2]\n",
        "        if len(token) > 5 and token.startswith(\"\\u064A\"):  # Yaa\n",
        "            for s2 in self.verb_su2:\n",
        "                if token.endswith(s2):\n",
        "                    return token[1:-2]\n",
        "        if len(token) > 4 and token.startswith(\"\\u0627\"):  # Alif\n",
        "            # Waaw Alif\n",
        "            if len(token) > 5 and token.endswith(\"\\u0648\\u0627\"):\n",
        "                return token[1:-2]\n",
        "            # Yaa\n",
        "            if token.endswith(\"\\u064A\"):\n",
        "                return token[1:-1]\n",
        "            # Alif\n",
        "            if token.endswith(\"\\u0627\"):\n",
        "                return token[1:-1]\n",
        "            # Noon\n",
        "            if token.endswith(\"\\u0646\"):\n",
        "                return token[1:-1]\n",
        "        # ^Yaa, Noon$\n",
        "        if len(token) > 4 and token.startswith(\"\\u064A\") and token.endswith(\"\\u0646\"):\n",
        "            return token[1:-1]\n",
        "        # ^Taa, Noon$\n",
        "        if len(token) > 4 and token.startswith(\"\\u062A\") and token.endswith(\"\\u0646\"):\n",
        "            return token[1:-1]\n",
        "\n",
        "\n",
        "    def verb_t2(self, token):\n",
        "        \"\"\"\n",
        "        stem the future prefixes and suffixes\n",
        "        \"\"\"\n",
        "        if len(token) > 6:\n",
        "            for s2 in self.pl_si2:\n",
        "                # ^Siin Taa\n",
        "                if token.startswith(self.verb_pr2[0]) and token.endswith(s2):\n",
        "                    return token[2:-2]\n",
        "            # ^Siin Yaa, Alif Noon$\n",
        "            if token.startswith(self.verb_pr2[1]) and token.endswith(self.pl_si2[0]):\n",
        "                return token[2:-2]\n",
        "            # ^Siin Yaa, Waaw Noon$\n",
        "            if token.startswith(self.verb_pr2[1]) and token.endswith(self.pl_si2[2]):\n",
        "                return token[2:-2]\n",
        "        # ^Siin Taa, Noon$\n",
        "        if (\n",
        "            len(token) > 5\n",
        "            and token.startswith(self.verb_pr2[0])\n",
        "            and token.endswith(\"\\u0646\")\n",
        "        ):\n",
        "            return token[2:-1]\n",
        "        # ^Siin Yaa, Noon$\n",
        "        if (\n",
        "            len(token) > 5\n",
        "            and token.startswith(self.verb_pr2[1])\n",
        "            and token.endswith(\"\\u0646\")\n",
        "        ):\n",
        "            return token[2:-1]\n",
        "\n",
        "\n",
        "    def verb_t3(self, token):\n",
        "        \"\"\"\n",
        "        stem the present suffixes\n",
        "        \"\"\"\n",
        "        if len(token) > 5:\n",
        "            for su3 in self.verb_suf3:\n",
        "                if token.endswith(su3):\n",
        "                    return token[:-3]\n",
        "        if len(token) > 4:\n",
        "            for su2 in self.verb_suf2:\n",
        "                if token.endswith(su2):\n",
        "                    return token[:-2]\n",
        "        if len(token) > 3:\n",
        "            for su1 in self.verb_suf1:\n",
        "                if token.endswith(su1):\n",
        "                    return token[:-1]\n",
        "\n",
        "\n",
        "    def verb_t4(self, token):\n",
        "        \"\"\"\n",
        "        stem the present prefixes\n",
        "        \"\"\"\n",
        "        if len(token) > 3:\n",
        "            for pr1 in self.verb_suf1:\n",
        "                if token.startswith(pr1):\n",
        "                    return token[1:]\n",
        "            if token.startswith(\"\\u064A\"):\n",
        "                return token[1:]\n",
        "\n",
        "\n",
        "    def verb_t5(self, token):\n",
        "        \"\"\"\n",
        "        stem the future prefixes\n",
        "        \"\"\"\n",
        "        if len(token) > 4:\n",
        "            for pr2 in self.verb_pr22:\n",
        "                if token.startswith(pr2):\n",
        "                    return token[2:]\n",
        "            for pr2 in self.verb_pr2:\n",
        "                if token.startswith(pr2):\n",
        "                    return token[2:]\n",
        "        return token\n",
        "\n",
        "\n",
        "    def verb_t6(self, token):\n",
        "        \"\"\"\n",
        "        stem the order prefixes\n",
        "        \"\"\"\n",
        "        if len(token) > 4:\n",
        "            for pr3 in self.verb_pr33:\n",
        "                if token.startswith(pr3):\n",
        "                    return token[2:]\n",
        "        return token\n",
        "\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    def __init__(self):\n",
        "        self.punctuations = re.compile(r'[\\u0021\\u002C\\u002D\\u002E\\u003A\\u003B\\u003F\\u066B\\u066C\\u06D4\\u061F\\u060C\\u000A\\u000D]')\n",
        "        emoj_file = io.open(_CORPUS_PATH + '/kaggle/input/nlp-paper/emoj.txt', encoding='utf-8')\n",
        "        self.emojis = emoj_file.read()\n",
        "        emoj_file.close()\n",
        "        self.stemmer = ARLSTem()\n",
        "        #self.linefeed = re.compile(r'[\\u000A\\u000D]')\n",
        "\n",
        "    def text_preprocessing(self, text):\n",
        "        # strip punctuation marks\n",
        "        text = self.punctuations.sub('', text)\n",
        "        text = self.remove_emojis(text)\n",
        "        text = self.remove_hashtag(text)\n",
        "        text = self.remove_letter_duplication(text)\n",
        "        text = self.stemming(text)\n",
        "        return text\n",
        "\n",
        "    def stemming(self, text):\n",
        "        words = text.split()\n",
        "        new_text = ''\n",
        "        for word in words:\n",
        "            new_text += self.stemmer.stem(word) + ' '\n",
        "        return new_text\n",
        "\n",
        "    def remove_hashtag(self, text):\n",
        "        return re.sub(r'#.+ ', ' ', text)\n",
        "\n",
        "    def remove_emojis(self, text):\n",
        "        for emo in self.emojis:\n",
        "            text = text.replace(emo, '')\n",
        "        return text\n",
        "\n",
        "    def remove_letter_duplication(self, text):\n",
        "        new_text = re.sub(r'\\u0627\\u0627+', '\\u0627', text) #ARABIC LETTER ALEF\n",
        "        new_text = re.sub(r'\\u0628\\u0628+', '\\u0628', new_text) #ARABIC LETTER BEH\n",
        "        new_text = re.sub(r'\\u0629\\u0629+', '\\u0629', new_text) #ARABIC LETTER TEH MARBUTA\n",
        "        new_text = re.sub(r'\\u062A\\u062A+', '\\u062A', new_text) #ARABIC LETTER TEH *\n",
        "        new_text = re.sub(r'\\u062B\\u062B+', '\\u062B', new_text) #ARABIC LETTER THEH\n",
        "        new_text = re.sub(r'\\u062C\\u062C+', '\\u062C', new_text) #ARABIC LETTER JEEM\n",
        "        new_text = re.sub(r'\\u062D\\u062D+', '\\u062D', new_text) #ARABIC LETTER HAH\n",
        "        new_text = re.sub(r'\\u062E\\u062E+', '\\u062E', new_text) #ARABIC LETTER KHAH\n",
        "        new_text = re.sub(r'\\u062F\\u062F+', '\\u062F', new_text) #ARABIC LETTER DAL\n",
        "\n",
        "        new_text = re.sub(r'\\u0630\\u0630+', '\\u0630', new_text) #ARABIC LETTER THAL\n",
        "        new_text = re.sub(r'\\u0631\\u0631+', '\\u0631', new_text) #ARABIC LETTER REH\n",
        "        new_text = re.sub(r'\\u0632\\u0632+', '\\u0632', new_text) #ARABIC LETTER ZAIN\n",
        "        new_text = re.sub(r'\\u0633\\u0633+', '\\u0633', new_text) #ARABIC LETTER SEEN\n",
        "        new_text = re.sub(r'\\u0634\\u0634+', '\\u0634', new_text) #ARABIC LETTER SHEEN\n",
        "        new_text = re.sub(r'\\u0635\\u0635+', '\\u0635', new_text) #ARABIC LETTER SAD\n",
        "        new_text = re.sub(r'\\u0636\\u0636+', '\\u0636', new_text) #ARABIC LETTER DAD\n",
        "        new_text = re.sub(r'\\u0637\\u0637+', '\\u0637', new_text) #ARABIC LETTER TAH\n",
        "        new_text = re.sub(r'\\u0638\\u0638+', '\\u0638', new_text) #ARABIC LETTER ZAH\n",
        "        new_text = re.sub(r'\\u0639\\u0639+', '\\u0639', new_text) #ARABIC LETTER AIN *\n",
        "        new_text = re.sub(r'\\u063A\\u063A+', '\\u063A', new_text) #ARABIC LETTER GHAIN\n",
        "\n",
        "        new_text = re.sub(r'\\u0641\\u0641+', '\\u0641', new_text) #ARABIC LETTER FEH\n",
        "        new_text = re.sub(r'\\u0642\\u0642+', '\\u0642', new_text) #ARABIC LETTER QAF\n",
        "        new_text = re.sub(r'\\u0643\\u0643+', '\\u0643', new_text) #ARABIC LETTER KAF\n",
        "        new_text = re.sub(r'\\u0644\\u0644+', '\\u0644', new_text) #ARABIC LETTER LAM\n",
        "        new_text = re.sub(r'\\u0645\\u0645+', '\\u0645', new_text) #ARABIC LETTER MEEM\n",
        "        new_text = re.sub(r'\\u0646\\u0646+', '\\u0646', new_text) #ARABIC LETTER NOON\n",
        "        new_text = re.sub(r'\\u0647\\u0647+', '\\u0647\\u0647\\u0647', new_text) #ARABIC LETTER HEH *\n",
        "        new_text = re.sub(r'(\\u0647\\u0627)+', '\\u0647\\u0647\\u0647', new_text) #ARABIC LETTER HEH-Alif *\n",
        "        new_text = re.sub(r'\\u0648\\u0648+', '\\u0648', new_text) #ARABIC LETTER WAW\n",
        "        new_text = re.sub(r'\\u0649\\u0649+', '\\u0649', new_text) #ARABIC LETTER ALEF MAKSURA\n",
        "        new_text = re.sub(r'\\u064A\\u064A+', '\\u064A', new_text) #ARABIC LETTER YEH\n",
        "        return new_text\n",
        "        \n",
        "################################################# Load the dataset#################################################\n",
        "prep = Preprocessing()\n",
        "latin_letters = re.compile(r'[abcdefghijklmnopqrstuvwxyz]+')\n",
        "\n",
        "df = pd.read_csv('/kaggle/input/nlp-paper/keras_test_pre_3c_trans_.txt', sep=',') \n",
        "x_text = df['content'].values.astype('U')\n",
        "y_test = df['label'].values\n",
        "for txt in range(len(x_text)):\n",
        "    x_text[txt] = prep.text_preprocessing(x_text[txt])\n",
        "\n",
        "\n",
        "df = pd.read_csv('/kaggle/input/nlp-paper/keras_train_pre_3c_trans_.txt', sep=',') \n",
        "x_train = df['content'].values.astype('U')\n",
        "y_train = df['label'].values\n",
        "for txt in range(len(x_train)):\n",
        "    x_train[txt] = prep.text_preprocessing(x_train[txt])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-29T19:48:39.417650Z",
          "iopub.execute_input": "2022-09-29T19:48:39.418025Z",
          "iopub.status.idle": "2022-09-29T19:48:42.743691Z",
          "shell.execute_reply.started": "2022-09-29T19:48:39.417991Z",
          "shell.execute_reply": "2022-09-29T19:48:42.742694Z"
        },
        "trusted": true,
        "id": "pNF8WjO-Tkc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extraction"
      ],
      "metadata": {
        "id": "AcE-CkQCTkdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################################################# Feature Extraction #################################################\n",
        "\n",
        "print(\"[INFO]  Start Feature extaction...\")\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "\n",
        "X_train_counts = count_vect.fit_transform(x_train)\n",
        "X_new_counts = count_vect.transform(x_text)\n",
        "\n",
        "\n",
        "trainX = tfidf_transformer.fit_transform(X_train_counts)\n",
        "testX = tfidf_transformer.transform(X_new_counts)\n",
        "\n",
        "print(\"[INFO]  Finished Feature extaction...\")\n",
        "\n",
        "trainX = trainX.todense()\n",
        "\n",
        "testX = testX.todense()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-29T19:48:42.745526Z",
          "iopub.execute_input": "2022-09-29T19:48:42.746253Z",
          "iopub.status.idle": "2022-09-29T19:48:43.200195Z",
          "shell.execute_reply.started": "2022-09-29T19:48:42.746215Z",
          "shell.execute_reply": "2022-09-29T19:48:43.198843Z"
        },
        "trusted": true,
        "id": "lvFmQN5ITkdE",
        "outputId": "edc4f9b2-1d1d-4248-f441-7210d127af54"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[INFO]  Start Feature extaction...\n[INFO]  Finished Feature extaction...\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Default**"
      ],
      "metadata": {
        "id": "BtnG6AzITkdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes=['normal','abusive','offensive']\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "print(\"################################################################################\")\n",
        "print(\"################################################################################\")\n",
        "\n",
        "################################################# Classification #################################################\n",
        "\n",
        "from cuml.svm import SVC\n",
        "\n",
        "# Use Chi2 stats to reduce dimensionality\n",
        "# and improve generalization\n",
        "\n",
        "\n",
        "# Use a RBF SVC\n",
        "\n",
        "\n",
        "svm = SVC(multiclass_strategy='ovr')\n",
        "\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "model = svm.fit(trainX, y_train)\n",
        "\n",
        "# Get Training time\n",
        "print(\"---Training time of Non-optimised SVM %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "start_time = time.time()\n",
        "predictions = model.predict(trainX)\n",
        "\n",
        "\n",
        "report = classification_report(y_train, predictions,digits=4)\n",
        "cm = confusion_matrix(y_train, predictions)\n",
        "\n",
        "print(report)\n",
        "\n",
        "acc_train = accuracy_score(y_train, predictions)\n",
        "print(\"Training accuracy of optimised SVM: %.4f%%\" % (acc_train * 100.0))\n",
        "\n",
        "\n",
        "f1 = f1_score(y_train, predictions,average='macro')\n",
        "print(\"Training f1 of ptimised SVM: %.4f%%\" % (f1 * 100.0))\n",
        "\n",
        "print(\"[INFO] evaluating...\")\n",
        "\n",
        "start_time = time.time()\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "# Get Testing time\n",
        "print(\"---Prediction time of Non-optimised SVM %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "report = classification_report(y_test, predictions,digits=4)\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "\n",
        "print(report)\n",
        "\n",
        "acc_test = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy of of optimised k and SVM: %.4f%%\" % (acc_test * 100.0))\n",
        "\n",
        "f1_mac_test = f1_score(y_test, predictions,average='macro')\n",
        "print(\"Testing macro f1 : %.4f%%\" % (f1_mac_test * 100.0))\n",
        "\n",
        "f1_mic_test = f1_score(y_test, predictions,average='micro')\n",
        "print(\"Testing micro f1 : %.4f%%\" % (f1_mic_test * 100.0))\n",
        "\n",
        "\n",
        "\n",
        "class_dict = {0: 'Normal',\n",
        "              1: 'Abusive',\n",
        "              2: 'Offensive'}\n",
        "\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=cm,figsize = (5, 5),\n",
        "    class_names=class_dict.values(),\n",
        ")\n",
        "\n",
        "ax.set_title('(B)')\n",
        "plt.savefig('cm_pre.png')\n",
        "\n",
        "plt.show()   \n",
        "\n",
        "\n",
        "\n",
        "plot_learning_curves(trainX, y_train, testX, y_test, model,scoring ='accuracy',style = 'seaborn',print_model=False)\n",
        "plt.savefig('lear_curve_pre.png')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZgqlV82iTkdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***** Combined optimised chi2 and svm**"
      ],
      "metadata": {
        "id": "uEWDzhn6TkdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes=['normal','abusive','offensive']\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "print(\"################################################################################\")\n",
        "print(\"################################################################################\")\n",
        "\n",
        "################################################# Classification #################################################\n",
        "\n",
        "from cuml.svm import SVC\n",
        "\n",
        "# Use Chi2 stats to reduce dimensionality\n",
        "# and improve generalization\n",
        "\n",
        "selec = SelectKBest(chi2, k=965)\n",
        "\n",
        "# Use a RBF SVC\n",
        "\n",
        "par = {'C': 215.3808095827046, 'gamma': 0.09614265972704511}\n",
        "\n",
        "svm = SVC(**par)\n",
        "\n",
        "pipeline = Pipeline(steps=[('selec', selec), ('svm', svm)])\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "model = pipeline.fit(trainX, y_train)\n",
        "\n",
        "# Get Training time\n",
        "print(\"---Training time of Non-optimised SVM %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "start_time = time.time()\n",
        "predictions = model.predict(trainX)\n",
        "\n",
        "\n",
        "report = classification_report(y_train, predictions,digits=4)\n",
        "cm = confusion_matrix(y_train, predictions)\n",
        "\n",
        "print(report)\n",
        "\n",
        "acc_train = accuracy_score(y_train, predictions)\n",
        "print(\"Training accuracy of optimised SVM: %.4f%%\" % (acc_train * 100.0))\n",
        "\n",
        "\n",
        "f1 = f1_score(y_train, predictions,average='macro')\n",
        "print(\"Training macro f1 of ptimised SVM: %.4f%%\" % (f1 * 100.0))\n",
        "\n",
        "f1 = f1_score(y_train, predictions,average='micro')\n",
        "print(\"Training micro f1 of ptimised SVM: %.4f%%\" % (f1 * 100.0))\n",
        "\n",
        "print(\"[INFO] evaluating...\")\n",
        "\n",
        "start_time = time.time()\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "# Get Testing time\n",
        "print(\"---Prediction time of Non-optimised SVM %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "report = classification_report(y_test, predictions,digits=4)\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "\n",
        "print(report)\n",
        "\n",
        "acc_test = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy of of optimised k and SVM: %.4f%%\" % (acc_test * 100.0))\n",
        "\n",
        "f1_mac_test = f1_score(y_test, predictions,average='macro')\n",
        "print(\"Testing macro f1 : %.4f%%\" % (f1_mac_test * 100.0))\n",
        "\n",
        "f1_mic_test = f1_score(y_test, predictions,average='micro')\n",
        "print(\"Testing micro f1 : %.4f%%\" % (f1_mic_test * 100.0))\n",
        "\n",
        "\n",
        "\n",
        "class_dict = {0: 'Normal',\n",
        "              1: 'Abusive',\n",
        "              2: 'Offensive'}\n",
        "\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=cm,figsize = (5, 5),\n",
        "    class_names=class_dict.values(),\n",
        ")\n",
        "\n",
        "ax.set_title('(E)')\n",
        "plt.savefig('cm_fs_hpo.png')\n",
        "\n",
        "plt.show()         \n",
        "                     \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plot_learning_curves(trainX, y_train, testX, y_test, model,scoring ='accuracy',style = 'seaborn',print_model=False)\n",
        "plt.savefig('lear_curve_hpofs.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "vpcWOVFRTkdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimised chi2**"
      ],
      "metadata": {
        "id": "ZOulHIiNTkdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes=['normal','abusive','offensive']\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "print(\"################################################################################\")\n",
        "print(\"################################################################################\")\n",
        "\n",
        "################################################# Classification #################################################\n",
        "\n",
        "from cuml.svm import SVC\n",
        "\n",
        "# Use Chi2 stats to reduce dimensionality\n",
        "# and improve generalization\n",
        "\n",
        "selec = SelectKBest(chi2, k = 1614)\n",
        "\n",
        "# Use a RBF SVC\n",
        "\n",
        "\n",
        "svm = SVC(multiclass_strategy='ovr')\n",
        "\n",
        "pipeline = Pipeline(steps=[('selec', selec), ('svm', svm)])\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "model = pipeline.fit(trainX, y_train)\n",
        "\n",
        "# Get Training time\n",
        "print(\"---Training time of Non-optimised SVM %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "start_time = time.time()\n",
        "predictions = model.predict(trainX)\n",
        "\n",
        "\n",
        "report = classification_report(y_train, predictions,digits=4)\n",
        "cm = confusion_matrix(y_train, predictions)\n",
        "\n",
        "print(report)\n",
        "\n",
        "acc_train = accuracy_score(y_train, predictions)\n",
        "print(\"Training accuracy of optimised SVM: %.4f%%\" % (acc_train * 100.0))\n",
        "\n",
        "\n",
        "f1 = f1_score(y_train, predictions,average='macro')\n",
        "print(\"Training macro f1 of ptimised SVM: %.4f%%\" % (f1 * 100.0))\n",
        "\n",
        "f1 = f1_score(y_train, predictions,average='micro')\n",
        "print(\"Training micro f1 of ptimised SVM: %.4f%%\" % (f1 * 100.0))\n",
        "\n",
        "print(\"[INFO] evaluating...\")\n",
        "\n",
        "start_time = time.time()\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "# Get Testing time\n",
        "print(\"---Prediction time of Non-optimised SVM %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "report = classification_report(y_test, predictions,digits=4)\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "\n",
        "print(report)\n",
        "\n",
        "acc_test = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy of of optimised k and SVM: %.4f%%\" % (acc_test * 100.0))\n",
        "\n",
        "f1_mac_test = f1_score(y_test, predictions,average='macro')\n",
        "print(\"Testing macro f1 : %.4f%%\" % (f1_mac_test * 100.0))\n",
        "\n",
        "f1_mic_test = f1_score(y_test, predictions,average='micro')\n",
        "print(\"Testing micro f1 : %.4f%%\" % (f1_mic_test * 100.0))\n",
        "\n",
        "\n",
        "\n",
        "class_dict = {0: 'Normal',\n",
        "              1: 'Abusive',\n",
        "              2: 'Offensive'}\n",
        "\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=cm,figsize = (5, 5),\n",
        "    class_names=class_dict.values(),\n",
        ")\n",
        "\n",
        "ax.set_title('(C)')\n",
        "plt.savefig('cm_fs.png')\n",
        "\n",
        "plt.show()         \n",
        "                   \n",
        "    \n",
        "plot_learning_curves(trainX, y_train, testX, y_test, model,scoring ='accuracy',style = 'seaborn',print_model=False)\n",
        "plt.savefig('chi2.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "yoTgXikdTkdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimised SVM**"
      ],
      "metadata": {
        "id": "cDdNaYAYTkdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes=['normal','abusive','offensive']\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "print(\"################################################################################\")\n",
        "print(\"################################################################################\")\n",
        "\n",
        "################################################# Classification #################################################\n",
        "\n",
        "from cuml.svm import SVC\n",
        "\n",
        "# Use Chi2 stats to reduce dimensionality\n",
        "# and improve generalization\n",
        "\n",
        "\n",
        "# Use a RBF SVC\n",
        "param = {'C': 118.43928118271889, 'gamma': 1.191377357362297}\n",
        "#param = {'C': 203.52073818431035, 'gamma': 0.27246196452098437}\n",
        "svm = SVC(**param,multiclass_strategy='ovr')\n",
        "\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "model = svm.fit(trainX, y_train)\n",
        "\n",
        "# Get Training time\n",
        "print(\"---Training time of Non-optimised SVM %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "start_time = time.time()\n",
        "predictions = model.predict(trainX)\n",
        "\n",
        "\n",
        "report = classification_report(y_train, predictions,digits=4)\n",
        "cm = confusion_matrix(y_train, predictions)\n",
        "\n",
        "print(report)\n",
        "\n",
        "acc_train = accuracy_score(y_train, predictions)\n",
        "print(\"Training accuracy of optimised SVM: %.4f%%\" % (acc_train * 100.0))\n",
        "\n",
        "\n",
        "f1 = f1_score(y_train, predictions,average='macro')\n",
        "print(\"Training f1 of ptimised SVM: %.4f%%\" % (f1 * 100.0))\n",
        "\n",
        "print(\"[INFO] evaluating...\")\n",
        "\n",
        "start_time = time.time()\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "# Get Testing time\n",
        "print(\"---Prediction time of Non-optimised SVM %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "report = classification_report(y_test, predictions,digits=4)\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "\n",
        "print(report)\n",
        "\n",
        "acc_test = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy of of optimised k and SVM: %.4f%%\" % (acc_test * 100.0))\n",
        "\n",
        "f1_mac_test = f1_score(y_test, predictions,average='macro')\n",
        "print(\"Testing macro f1 : %.4f%%\" % (f1_mac_test * 100.0))\n",
        "\n",
        "f1_mic_test = f1_score(y_test, predictions,average='micro')\n",
        "print(\"Testing micro f1 : %.4f%%\" % (f1_mic_test * 100.0))\n",
        "\n",
        "\n",
        "class_dict = {0: 'Normal',\n",
        "              1: 'Abusive',\n",
        "              2: 'Offensive'}\n",
        "\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=cm,figsize = (5, 5),\n",
        "    class_names=class_dict.values(),\n",
        ")\n",
        "\n",
        "ax.set_title('(D)')\n",
        "\n",
        "plt.savefig('cm_hpo.png')\n",
        "\n",
        "plt.show()         \n",
        "                     \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plot_learning_curves(trainX, y_train, testX, y_test, model,scoring ='accuracy',style = 'seaborn',print_model=False)\n",
        "plt.savefig('lear_curve_svm.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "C8CCLCdMTkdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes=['normal','abusive','offensive']\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "print(\"################################################################################\")\n",
        "print(\"################################################################################\")\n",
        "\n",
        "################################################# Classification #################################################\n",
        "\n",
        "from cuml.svm import SVC\n",
        "\n",
        "# Use Chi2 stats to reduce dimensionality\n",
        "# and improve generalization\n",
        "\n",
        "\n",
        "# Use a RBF SVC\n",
        "\n",
        "\n",
        "svm = SVC()\n",
        "\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "model = svm.fit(trainX, y_train)\n",
        "\n",
        "# Get Training time\n",
        "print(\"---Training time of Non-optimised SVM %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "start_time = time.time()\n",
        "predictions = model.predict(trainX)\n",
        "\n",
        "\n",
        "report = classification_report(y_train, predictions,digits=4)\n",
        "cm = confusion_matrix(y_train, predictions)\n",
        "\n",
        "print(report)\n",
        "\n",
        "acc_train = accuracy_score(y_train, predictions)\n",
        "print(\"Training accuracy of optimised SVM: %.4f%%\" % (acc_train * 100.0))\n",
        "\n",
        "\n",
        "f1 = f1_score(y_train, predictions,average='macro')\n",
        "print(\"Training f1 of ptimised SVM: %.4f%%\" % (f1 * 100.0))\n",
        "\n",
        "print(\"[INFO] evaluating...\")\n",
        "\n",
        "start_time = time.time()\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "# Get Testing time\n",
        "print(\"---Prediction time of Non-optimised SVM %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "report = classification_report(y_test, predictions,digits=4)\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "\n",
        "print(report)\n",
        "\n",
        "acc_test = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy of of optimised k and SVM: %.4f%%\" % (acc_test * 100.0))\n",
        "\n",
        "f1_mac_test = f1_score(y_test, predictions,average='macro')\n",
        "print(\"Testing macro f1 : %.4f%%\" % (f1_mac_test * 100.0))\n",
        "\n",
        "f1_mic_test = f1_score(y_test, predictions,average='micro')\n",
        "print(\"Testing micro f1 : %.4f%%\" % (f1_mic_test * 100.0))\n",
        "\n",
        "\n",
        "\n",
        "class_dict = {0: 'Normal',\n",
        "              1: 'Abusive',\n",
        "              2: 'Offensive'}\n",
        "\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=cm,figsize = (5, 5),\n",
        "    class_names=class_dict.values(),\n",
        ")\n",
        "\n",
        "\n",
        "plt.savefig('cm_non.png')\n",
        "\n",
        "plt.show()   \n",
        "\n",
        "from mlxtend.plotting import plot_learning_curves\n",
        "\n",
        "\n",
        "\n",
        "plot_learning_curves(trainX, y_train, testX, y_test, model,scoring ='accuracy',style = 'seaborn',print_model=False)\n",
        "plt.savefig('lear_curve_non.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "VyFZsxo7TkdQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}